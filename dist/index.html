<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Agency - Start Bootstrap Theme</title>
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.3/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
            <div class="container">
                <!--<a class="navbar-brand" href="#page-top"><img src="assets/img/navbar-logo.svg" alt="..." /></a>-->
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars ms-1"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav text-uppercase ms-auto py-4 py-lg-0">
                        <li class="nav-item"><a class="nav-link" href="#motivation">Motivation</a></li>
                        <li class="nav-item"><a class="nav-link" href="#methodology">Methodology</a></li>
                        <li class="nav-item"><a class="nav-link" href="#results">Results</a></li>
                        <li class="nav-item"><a class="nav-link" href="#key-takeaways">Key Takeaways</a></li>
                        <li class="nav-item"><a class="nav-link" href="#future-work">Future Work</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Masthead-->
        <header class="masthead">
            <div class="container">
                <div class="masthead-subheading">Data Plus 2021</div>
                <div class="masthead-heading text-uppercase">Creating AI for Artificial Worlds</div>
                <p class="text-white-75 font-weight-light mb-5" style="font-size: 30px">Frankie Willard, Alexander Kumar, Caroline Tang</p>
                <a class="btn btn-primary btn-xl text-uppercase" href="#services">Tell Me More</a>
            </div>
        </header>
        <!-- Motivation -->
        <section class="page-section" id="motivation">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="col-lg-12 text-center">
                        <h2 class="section-heading text-uppercase">Motivation</h2>

                        <hr class="divider light my-4" />
                    </div>
                </div>
                <div class="row justify-content-center">
                    <div class="row">
                        <div class="col-md-12">
                            <h3>Energy Access Planning</h3>
                            <p class="text-muted" style="text-align: justify">
                                Access to electricity is becoming increasingly critical, especially for promoting economic development, social equity,
                                and improving quality of life. Further, it has been shown that electricity access is correlated with improvements in income,
                                education, maternal mortality, and gender equality.
                                Yet, worldwide, 16% of the global population, or <a href="https://energyeducation.ca/encyclopedia/Access_to_electricity">approximately 1.2 billion people, still don’t have access to electricity in their
                                homes.</a> This map from the World Bank in 2017 highlights the uneven distribution of energy access, with the majority of those without
                                electricity access concentrated in sub-Saharan Africa and Asia.
                            </p>
                        </div>
                    </div>
                    <div class="row">
                        <div class="col-md-12">
                            <img src="assets/img/motivation/Global Access to Electricity.png" class="img-responsive" alt=""style="width:100%; height:70%">
                            <caption><i>Image from: <a href="https://www.visualcapitalist.com/mapped-billion-people-without-access-to-electricity/">
                                https://www.visualcapitalist.com/mapped-billion-people-without-access-to-electricity/</a></i></caption>

                            <p style="margin-top: 2.5em" class="text-muted"  style="text-align: justify">
                                One of the first steps in improving energy access is acquiring comprehensive data on the existing energy infrastructure in an area.
                                This includes information on the kind, quality, and location so that energy developers and policymakers can then strategically and
                                optimally deploy energy resources. This information is key for helping them make decisions about where to prioritize development,
                                and whether they should use grid extension, micro/minigrid development,  or offgrid options to bring electricity to communities.
                                <br><br>
                                However, this critical information for expanding energy accessibility is often unattainable or low quality.
                                One potential solution to this issue is to automate the process of mapping energy infrastructure in satellite imagery. Using deep
                                learning, we can input satellite imagery into an object detection model and make predictions about the characteristics and contents
                                of the energy structure in the region featured in the image.
                            </p>
                        </div>
                    </div>
                </div>

                <div class="row text-center">
                    <h3 class="service-heading">Object Detection</h3>
                    <div class="row">

                        <div class="col-md-6">
                            <img src="assets/img/motivation/object detection explanation.png" class="img-responsive" alt="" style="width:100%">
                            <caption><i>Image from: <a href="https://www.researchgate.net/figure/Object-detection-in-a-dense-scene_fig4_329217107">
                                ResearchGate</a></i></caption>
                        </div>


                        <div class="col-md-6">
                            <p class="text-muted" style="text-align: justify;">
                                Our project has a particular emphasis on object detection, as we seek to improve the detection of energy infrastructure
                                in different terrains as a part of expanding energy access data. Object detection is a combination of classification
                                (identifying the correct object) and object localisation (identifying the location of a given object). Object detection
                                models analyze the scenery of photos and generate bounding boxes around each object in the image. In doing so, it classifies
                                each object and assigns a confidence score based on the accuracy of its prediction. The model predicted that each of the green,
                                yellow, and pink boxes in the image on the left would indicate different objects, being a handbag, a car, and a handbag. Based
                                on examples provided to it, the model learns how to predict these boxes and classes. We refer to these labeled images as ground
                                truth as they contain boxes that denote every object's class and the location within the image.
                            </p>
                        </div>
                    </div>
                </div>

                <br><br>

                <div class="row text-center">
                    <h3 class="service-heading">Applying Deep Learning to Overhead Imagery</h3>
                        <div class="row">

                            <div class="col-md-12 my-auto">
                                <img src="assets/img/motivation/apply deep learning model to overhead imagery.png" class="img-responsive" alt="apply deep learning" style="width:90%">
                            </div>
                        </div>

                        <div class="row">
                            <div class="col-md-12 my-auto">
                                <p class="text-muted" style="text-align: justify;">
                                    After training our object detection model, we can apply it to a collection of overhead imagery to locate and classify different energy infrastructure across entire regions.
                                    In our experiments, we test our ability to detect wind turbines to maintain consistency with previous experiments.
                                    While we could demonstrate energy infrastructure detection for any number of types of electricity infrastructure, wind turbines were chosen
                                    due to their relatively homogeneous nature as opposed to different power plants and other energy infrastructure.
                                    Ultimately, the methods used to improve object detection of energy infrastructure will be expanded to more energy
                                    infrastructure, however, limiting the infrastructures tested helps to quickly provide performance benchmarks.
                                </p>
                            </div>
                        </div>
                </div>

                <br><br>

                <div class="row text-center">

                    <h3 class="service-heading">Challenges with Object Detection</h3>

                    <br>

                    <div class="row">
                        <div class="col-md-12">
                            <h4>Problem 1: Lack of labeled data for rare objects</h4>
                            <p class="text-muted" style="text-align: justify;">
                                While the potential of object detection seems promising, it presents two main challenges. The first
                                is that properly training the object detection model requires thousands of already labelled images.
                                Because many types of energy infrastructure are rare objects, obtaining and annotating such a large
                                quantity of satellite images featuring these infrastructures manually is expensive.
                            </p>
                        </div>

                        <!--<div class="col-md-6">
                            <img src="assets/img/motivation/wind_turbines_rarity.png" class="img-responsive" alt=""style="width:80%; height:80%">
                        </div>
                      </div>
                      <div class="row">-->

                        <div class="col-md-12">
                            <h4>Problem 2: Domain adaptation</h4>
                            <p class="text-muted" style="text-align: justify;">
                                The second challenge we face is that these models are not capable of generalizing across dissimilar images yet.
                                What this means is that if we train our model on a collection of images from one region, featuring images with
                                similar background geographies, the model will then be able to perform fairly well on other images with those same
                                physical background characteristics. However, if we then try to input images from a different region with different
                                geographic characteristics, the models’ performance becomes significantly worse.
                            </p>
                        </div>

                        <div class="col-md-12">
                            <img src="assets/img/motivation/domain_adaptation_updated.png" class="img-responsive" alt=""style="width:90%; height:90%"><br>
                            <caption><i>Figure: Example of the domain adaptation challenge, for a model trained on images with geographical features of the images in the
                                source domain (forest & grasslands) that underperforms when tested on images with different background in the target domain (desert)</i></caption>
                        </div>
                    </div>
                </div>

                <br><br>

                <div class="row text-center">
                    <h3 class="service-heading">Proposed solution: synthetic imagery</h3>
                    <div class="row">
                        <div class="col-md-12 my-auto">

                            <p class="text-muted" style="text-align: justify;">
                                Our proposed solution to address these two problems is to introduce synthetic images into our training dataset.
                                The synthetic images supplement the original real satellite imagery dataset to create a larger dataset to train
                                and test on. We generate these synthetic images by cropping the energy infrastructure out of satellite images
                                and placing them on top of a real image without any energy infrastructure from one of the target geographic domains.
                            </p>
                        </div>
                        <div class="col-md-12">
                            <img src="assets/img/motivation/proposed_solution.jpg" class="img-responsive" alt=""style="width:90%; height:90%"><br>
                            <caption><i>Figure: Synthetic imagery generation process overview</i></caption>
                        </div>
                    </div>
                </div>




                <div class="row">
                    <div class="col-md-12">
                        <h3 class="service-heading">Previous work</h3>
                        <p class="text-muted" style="text-align: justify;">
                            For five years, the Duke Energy Data Analytics Lab has worked on developing deep learning models that identify energy infrastructure,
                            with an end goal of generating maps of power systems and their characteristics that can aid policymakers in implementing effective
                            electrification strategies. In 2015-16, researchers created a model that can detect solar photovoltaic arrays with high accuracy <a
                            href="https://bassconnections.duke.edu/project-teams/energy-data-analytics-lab-2015-2016">[2015-16
                            Bass Connections Team]</a>.
                            In 2018-19, this model was improved to identify different types of transmission and distribution energy infrastructures,
                            including power lines and transmission towers <a
                            href="https://bassconnections.duke.edu/project-teams/energy-data-analytics-lab-energy-infrastructure-map-world-through-satellite-data-2018">[2018-19
                            Bass Connections Team]</a>. Last year's project focused on increasing the adaptability of detection models
                            across different geographies by creating realistic synthetic imagery <a
                            href="https://bass-connections-2019.github.io/">[2019-20
                            Bass Connections Team]</a>.<br/><br/>
                            In 2020-2021, the Bass Connections project team extended this work, trying to improve the model’s ability to accurately detect rare objects in diverse
                            locations. After collecting satellite imagery from various domains and clustering them, they experimented with generating synthetic imagery by taking
                            satellite images featuring no energy infrastructure and placing 3D models of the object of interest on top of the image, and capturing a photo that
                            mimicked the appearance of a satellite image.
                            In our project, we build upon this progress and try to improve the 2020 Bass Connections team's ability to enhance energy infrastructure detection
                            in new, diverse locations.
                        </p>
                    </div>
                    <!--<div class="col-md-6">
                      <img src="assets/img/overview_OLD/energy_information.png" class="img-responsive center" alt=""
                      style="width:100%">
                    </div>-->
                </div>


            </div>
        </section>


        <!-- Methodology -->
        <section class="page-section" id="methodology">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="col-lg-12 text-center">
                        <h2 class="section-heading text-uppercase">Methodology</h2>
                        <hr class="divider light my-4" />
                        <div class="row">
                            <div class="col-md-12 my-auto">
                                <p class="text-muted" style="text-align: justify;">
                                    Below is a description of the experiments we conducted to evaluate if adding synthetic images to an object detection
                                    algorithm enhances its performance across geographic domains. After gathering real images and generating synthetic images,
                                    we can construct two datasets. The first dataset includes only real imagery, while the second dataset includes both real
                                    and synthetic images. We can train an object detection model on the first dataset, test it, and then repeat the process with
                                    the second dataset, comparing the results. If the model performs better when trained on a dataset with synthetic imagery, we
                                    can conclude that the synthetic imagery aids the model's performance. Given the Bass Connections previous work creating a synthetic
                                    dataset using CityEngine, we can also compare our synthetic dataset’s performance against theirs, to determine which synthetic dataset
                                    best improves energy infrastructure detection.
                                </p>
                            </div>
                            <div class="col-md-12 my-auto">
                                <img src="assets/img/methodology/experimental%20design%20setup.jpg" class="img-responsive" alt=""style="width:100%">
                            </div>
                        </div>
                        <!--<hr class="divider light my-4" />
                        <h2 class="service-heading text-left">Collecting Real Imagery</h2>
                        <div class="row">
                            <div class="col-md-12 my-auto">
                                <p class="text-muted" style="text-align: justify;"> For our overhead imagery of wind turbines, we chose to sample them from
                                    <a href="https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/">the National Agriculture Imagery Program</a>.
                                    This imagery covers a large part of the US and is very high resolution, making it great for our experiments.
                                    We collected imagery in three different regions that we called Northwest, Northeast, and Eastern Midwest. We noticed
                                    differences in the visual appearance of the background in the images collected in these three regions:
                                </p>
                            </div>
                            <div class="col-md-4 my-auto">
                                <h5 class="service-heading">Northwest</h5>
                                <dl>
                                    <dd class="text-muted">- Hue is mostly brown</dd>
                                    <dd class="text-muted">- Mostly desert and grassland</dd>
                                </dl>
                            </div>
                            <div class="col-md-4 my-auto">
                                <h5 class="service-heading">Northeast</h5>
                                <dl>
                                    <dd class="text-muted">- Hue is very green</dd>
                                    <dd class="text-muted">- Mostly forests</dd>
                                </dl>
                            </div>
                            <div class="col-md-4 my-auto">
                                <h5 class="service-heading">Eastern Midwest</h5>
                                <dl>
                                    <dd class="text-muted">- Hue is mostly green, some brown</dd>
                                    <dd class="text-muted">- Primarily farmland</dd>
                                </dl>
                            </div>
                            <div class="col-md-12 my-auto">
                                <img src="assets/img/methodology/Collecting Overhead Imagery.png" class="img-responsive" alt="" style="width:80%"><br>
                                <caption><i>Figure: Each dot represents a single image that we collected.</i></caption>
                            </div>
                        </div>
                        <br>
                        <br>
                        <div class="row">
                            <div class="col-md-12 my-auto">
                                <p class="text-muted" style="text-align: justify;"> Below we can see the regions splits by which states they include,
                                    as well as the number of images we collected for each region.
                                </p>
                                <img src="assets/img/methodology/Where Our Data is From.png" class="img-responsive" alt="" style="width:70%"><br>
                                <caption><i>Figure: Map of the U.S. showing the U.S. states and number of images we collected in each region</i></caption>
                            </div>
                        </div>-->
                    </div>
                </div>


                <hr class="divider light my-4" />
                <div class="row">
                    <div class="col-md-12 my-auto">
                        <h2 class="service-heading">Creating Synthetic Imagery</h2>
                        <br>
                        <div class="row">
                            <div class="col-md-6 my-auto">
                                <h5 class="service-heading">GANs Overview</h5>
                                <p class="text-muted" style="text-align: justify;"> Generative Adversarial Networks (GANs) are a method of generative modeling.
                                    The concept behind GANs is a zero sum game between two Neural Networks- a generator network and a discriminator network.
                                    While the generator attempts to create images that are as realistic as possible, the discriminator tries to determine if
                                    those images are real or fake (generated by the GAN). This novel approach to generative modeling has seen a rapid increase in
                                    usage in the various scientific domains, due to its ability to generate photorealistic images for tasks including data augmentation,
                                    creating art, image to image translation, image harmonization, and image super-resolution. [Source]
                                </p>
                            </div>
                            <div class="col-md-6 my-auto">
                                <img src="assets/img/methodology/gan.png" class="img-responsive" alt="" style="width:100%">
                                <caption class="text-muted"><i>Figure: Bounding box size distribution of turbines in real imagery.</i></caption>
                            </div>
                        </div>
                        <br><br>
                        <div class="row">
                            <div class="col-md-6 my-auto">
                                <img src="assets/img/methodology/gp_gan.png" class="img-responsive" alt="" style="width:100%">
                                <caption class="text-muted"><i>Figure: Bounding box size distribution of turbines in real imagery.</i></caption>
                            </div>
                            <div class="col-md-6 my-auto">
                                <h5 class="service-heading">GANs for Image Blending</h5>
                                <p class="text-muted" style="text-align: justify;"> In trying to generate synthetic imagery with real wind turbines
                                    in new terrains, our problem presented the need for image harmonization and image blending. This consists of matching
                                    the visual appearance/style of the wind turbine and geographical background images when blending them into a single image.
                                    Given the GANs state-of-the-art performance in “GP-GAN: Towards Realistic High-Resolution Image Blending” by Huikai Wu, et
                                    al., we chose to investigate the potential for GANs to realistically develop our synthetic imagery dataset. [Source]
                                </p>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-12 my-auto">
                        <br><br>
                        <h2 class="service-heading">Synthetic Imagery with GP GAN Pipeline</h2>
                        <img src="assets/img/methodology/process.jpg" class="img-responsive" alt="" style="width:100%">
                    </div>
                </div>

                <br><br>
                <!--<div class="row">
                    <div class="col-md-4 my-auto">
                        <p class="text-muted" style="text-align: justify;"> We can repeat this process but remove the background images, and color
                            in the turbine models as black to retrieve information on where the turbine models are located. The black pixels in these
                            images can be automatically grouped together to locate the wind turbines and create a formatted label that contains the
                            bounding box around each turbine model.
                        </p>
                    </div>
                    <div class="col-md-8 my-auto">
                        <img src="assets/img/methodology/Generating Annotations.png" class="img-responsive" alt="" style="width:100%">
                        <caption><i>Figure: Side-by-side of an RGB image with its corresponding black-and-white label.</i></caption>
                    </div>
                </div>-->
                <br><br>
                <div class="row">
                    <div class="col-md-12 my-auto">
                        <p class="text-muted" style="text-align: justify;">
                            The synthetic images produced by the GP GAN are a quick and cost-effective solution to labeled datasets that
                            are incommensurate with the needed training set size (for adequate performance). To produce our images, we simply need
                            a few images with each energy infrastructure (already in training set) and background images. These images do not have
                            to be labeled, and thus are much more abundant and available for easy use.
                            We can then make simple crops and bounding boxes for sampled real wind turbines within minutes.
                            Our automatic image augmenter then randomly samples sizes, locations, and rotations to generate hundreds of
                            source images in seconds. Our generated sources (size = m) can be matched with any and all destinations (size = n)
                            to create m times n output images that are produced by the GP GAN. Each image is blended in approximately 7 seconds, making
                            our data pipeline an incredibly quick and resource efficient solution for data scarcity.
                            For reference, if we generate 10 source images per destination image and download 100 destination images, we can create a dataset
                            that includes 1000 images in less than 2 hours that includes a diversity in wind turbine location, rotation, size,
                            and geographical background.
                            Below are some example synthetic images created
                            from a variety of background images.
                        </p>
                    </div>
                    <div class="col-md-12 my-auto">
                        <img src="assets/img/methodology/synthetic%20image%20examples.jpg" class="img-responsive" alt="" style="width:80%;"><br>
                        <caption class="text-muted"><i>Figure: Our synthetic images contain a variety of background images and camera angles. They look fairly similar to our real imagery.</i></caption>
                    </div>
                </div>
                <br><br>
                <div class="row">
                    <div class="col-md-12 my-auto">
                        <h3 class="service-heading">Synthetic Imagery Design Considerations</h3>
                        <p class="text-muted" style="text-align: left;"> The design of the synthetic imagery is important because the closer the
                            synthetic imagery is to the real imagery, the more the synthetic imagery will improve our performance when adding it to our
                            training set.
                        </p>
                    </div>
                </div>
                <br>
                <div class="row">
                    <div class="col-md-6 my-auto">
                        <img src="assets/img/methodology/size_distribution.png" class="img-responsive" alt="" style="width:100%">
                        <caption class="text-muted"><i>Figure: Bounding box size distribution of turbines in real imagery.</i></caption>
                    </div>
                    <div class="col-md-6 my-auto">
                        <h5 class="service-heading">Location, Rotation, and Size of the Synthetic Turbines</h5>
                        <p class="text-muted" style="text-align: justify;">
                            The first step of our image generation pipeline is our image augmenter, which leads us to consider
                            the location, size, and rotation of our synthetic turbines.
                            The previous Bass Connections team modeled the size distribution of the CityEngine turbines after
                            the size distribution of the real turbines.
                            To ensure a controlled experiment, we sampled their location and sizing information to match the real distributions.
                            While no rotation data was previously stored, we randomized the rotation of every windmill to allow the object detection model
                            to become familiar with wind turbines from various angles and views.

                        </p>
                    </div>
                </div>
                <br>
                <!--<div class="row">
                    <div class="col-md-6 my-auto">
                        <h5 class="service-heading">Angle of the Camera</h5>
                        <p class="text-muted" style="text-align: justify;"> The next decision we had to make about the synthetic imagery design was
                            the angle of the simulated camera when we are capturing photos. We noticed that in the real imagery,
                            some of the images were captured at an angle. In the real overhead imagery, you can see the pole of
                            the turbine due to the angle of the camera. We observed that about half of the real images were taken from directly above (90 degrees),
                            and the rest were taken at a variety of angles that were between 60-90 degrees. In our synthetic image generation process, we
                            chose half of the time to take the image from directly above. The other half of the time, we would use a
                            randomly chosen camera angle between 60 and 90 degrees.
                        </p>
                    </div>
                    <div class="col-md-6 my-auto">
                        <img src="assets/img/methodology/Angle of Camera 2.png" class="img-responsive" alt="" style="width:100%">
                        <caption class="text-muted"><i>Figure: Bounding box size distribution of turbines in real imagery.</i></caption>
                    </div>
                </div>-->
                <br>
                <div class="row">
                    <div class="col-md-6 my-auto">
                        <h5 class="service-heading">Which Background Images to Use</h5>
                        <p class="text-muted" style="text-align: justify;">
                            Additionally, we had to choose which background images to have placed under our synthetic wind
                            turbine models. In corresponding with the Bass Connections CityEngine experiments, we chose to
                            use background imagery close to the real images in our testing set to maximize the similarity
                            of our synthetic imagery with the target data.
                            This methodology is consistent with real scenarios, as we will likely have access to unlabelled imagery
                            or have the ability to collect unlabelled imagery from around the region we wish to test on for use as background images.
                            Given the lack of manual labeling and filtering required as well as our ability to generate many sources to blend with each background image, this background data collection
                            would ideally not be too time consuming.
                            Using the background images close to our testing locations allows us to estimate the potential performance
                            increase that the synthetic data can provide without introducing confounding variables such as a mismatch
                            between the synthetic background image domain and the target domain (makes it difficult to attribute poor performance to
                            the geographical background or synthetic data generation).
                        </p>
                    </div>
                    <div class="col-md-6 my-auto">
                        <img src="assets/img/methodology/backgrounds.png" class="img-responsive" alt="" style="width:100%">
                        <caption class="text-muted"><i>Figure: Test image and nearby collected background image.</i></caption>
                    </div>
                </div>

                <!--<hr class="divider light my-4" />-->

                <div class="row">
                    <div class="col-md-12 my-auto">
                        <!--
                        <h2 class="service-heading">Constructing the Datasets</h2>

                        <div class="row">
                            <div class="col-md-12 my-auto">
                                <h5 class="service-heading">Clustering and Stratified Sampling</h5>
                                <p class="text-muted" style="text-align: justify;">
                                    Because our self-defined geographic regions are wide and diverse, it's important for our training and testing datasets
                                    to be representative of a given geographic region. To increase homogeneity within a region (we define a region as a "domain"), we clustered
                                    within each region and then performed stratified sampling from each cluster with proportional allocation to construct our 1:1 train:validation ratio (1 training image for each validation image) constrained
                                    baseline datasets, including 100 images total across the clusters within each domain. Each point represents an image, and each color represents a cluster. Points that are in the same cluster are more similar to each other than points that are in different clusters. </p>
                            </div>
                            <div class="col-md-12 my-auto">
                                <p style="text-align:center;">
                                    <img src="assets/img/methodology/Clustering and Stratified Sampling.png" class="img-responsive" alt="" class="center" style="width:80%; height:70%">
                                </p>
                                <br>
                                <caption class="text-muted"><i>Figure: DBSCAN clustering within each geographic region and then stratified random sampling from each cluster to construct our training and testing datasets. Each dot represents a real imge, and each color represents a different cluster. For each cluster, the sample size is proportional to the cluster size, so larger clusters will have more images in both training and testing data. </i></caption>
                            </div>
                        </div>

                        <h5 class="service-heading">Optimizing the Ratio of Real to Synthetic Data</h5>
                        <div class="row">

                            <div class="col-md-6 my-auto">
                                <img src="assets/img/methodology/Ratio Test Experimental Design.png" class="img-responsive" alt="Ratio Test Experimental Design" style="width:100%">
                            </div>

                            <div class="col-md-6 my-auto">
                                <p class="text-muted" style="text-align: justify;">
                                    To construct our baseline and add synthetic data, we need to figure out what ratio of real to synthetic data yields
                                    the largest gain in performance (if any). If we add too much synthetic data, then we run the risk of overfitting to synthetic data. If we add too little synthetic data, then it will have little impact on performance. To find this ratio, we designed an experiment, where we test ratios of 1:0, 1:0.5, 1:0.75,
                                    1:1, and 1:2 real to synthetic ratios. After conducting these experiments, we found that 1:0.75 yields the greatest performance
                                    as measured by average precision. Therefore, we design our experiments using the 1:0.75 ratio.  </p>
                            </div>
                        </div>-->

                        <hr class="divider light my-4" />
                        <div class="row">
                            <div class="col-md-12 my-auto">
                                <h2 class="service-heading">Experimental Setup</h2>

                                <br>

                                <h5 class="service-heading">Overview</h5>
                                <div class="row">
                                    <div class="col-md-6 my-auto">
                                        <p class="text-muted" style="text-align: justify;">
                                            To evaluate the potential of synthetic imagery in improving the performance of object detection, we set up within and cross domain experiments, where a domain is defined as a specific geographic region. The source domain refers to the region that the real training data comes from, while the target domain refers to the region that the object detection model is applied to. These two types of experiments each correspond to a potential real-world situation one might encounter, and help us to evaluate the potential performance of the object detection model in each of these situations.
                                            <br><br>
                                            In the context of energy access planning, the ultimate goal of this project is to utilize object detection in various regions of the world where energy access is extremely limited and information on existing energy infrastructure is not readily available. Thus, the object detection model must be able to generalize well across different images despite labeled real satellite imagery most likely being limited.

                                        </p>
                                    </div>

                                    <div class="col-md-6 my-auto">
                                        <img src="assets/img/methodology/Overall Domain Map.png" class="img-responsive" class="center" alt="" style="width:100%; height:150%">
                                        <br>
                                        <caption class="text-muted"><i>Figure: Overall Experiment Setup. In within-domain experiments, the target domain (Northwest) remains the same geographic region as the source domain (Northwest). In cross-domain experiments, the target domain (Northeast) has no labeled real data, so the model is trained on a different source domain (Northwest) and then applied to the target domain. Orange color denotes a source domain, whereas blue color denotes a target domain.</i></caption>
                                    </div>
                                </div>

                                <div class="row">
                                    <div class="col-md-6 my-auto">
                                        <img src="assets/img/methodology/Pairwise Cycle.png" class="img-responsive" alt="Pairwise Experiment Setup" style="width:100%">
                                        <caption class="text-muted"><i>Figure: Pairwise experiment setup. The arrow tails point toward the source domain (where real training data comes from), whereas the arrow heads point toward the target domain (where the model will be tested on). Bi-directional arrows indicate each domain serves as the source for testing the other two domains, and in a separate experiment, the same domain serves as the target to be tested using model trained on the other domains. </i></caption>
                                    </div>

                                    <div class="col-md-6 my-auto">
                                        <p class="text-muted" style="text-align: justify;">
                                            Our within-domain experiments, where the source and target domains are within the same geographic region, will help us to evaluate the potential for synthetic imagery to supplement limited real training data.
                                            <br><br>
                                            However, as mentioned previously, one of the key challenges that object detection presents is its poor performance when applied to data that looks significantly different from the data on which it is trained. Thus, the cross-domain experiments reflect the potential situation where there exists no data at all from the target domain, and thus the object detection model must be trained on data from an entirely different region. For this experiment, the synthetic data that is used will come from the target region, but the real images will come from a source region, different from the target.
                                        </p>

                                    </div>

                                </div>

                                <br><br>
                                <div class="row">
                                    <h5 class="service-heading">Optimizing the Ratio of Real to Synthetic Data</h5>
                                    <div class="row">

                                        <div class="col-md-6 my-auto">
                                            <p class="text-muted" style="text-align: justify;">
                                                In constructing our experimental datasets, we need to figure out what ratio of real to synthetic data yields
                                                the largest gain in performance (if any). Adding too much synthetic data could lead to overfitting to synthetic data and any irregularities within
                                                the synthetic data or differences with regular images would be exacerbated such the object detection model may perform worse.
                                                However, adding too little synthetic data will have a negligible effect on performance.
                                                The 2019-2020 Bass Connections team designed an experiment in which they tested ratios of 1:0, 1:0.5, 1:0.75,
                                                1:1, and 1:2 real to synthetic ratios. After conducting these experiments, they found that 1:0.75 yields the greatest performance
                                                as measured by average precision. Therefore, to maintain similarity to the Bass Connections team, we design our experiments using the 1:0.75 ratio.
                                                This ratio allows for fair comparison of our synthetic data generation with the Bass Connections team, however, in the future, we
                                                would like to experiment with different ratios with our synthetic data generation process to find the optimal ratio.
                                            </p>
                                        </div>

                                        <div class="col-md-6 my-auto">
                                            <img src="assets/img/methodology/Ratio Test Experimental Design.png" class="img-responsive" alt="Ratio Test Experimental Design" style="width:100%">
                                        </div>
                                    </div>


                                    <br>
                                    <h5 class="service-heading">Design Setup</h5>
                                    <p class="text-muted" style="text-align: justify;">
                                        Having sampled our data and found the optimal real to synthetic ratio, our final datasets for each region is: </p>
                                    <ul>
                                        <li class="text-muted" style="text-align: justify;"><b>Baseline: </b> Train on 100 Real Non-Target Images, Test on 100 Target Domain Images</li>
                                        <li class="text-muted" style="text-align: justify;"><b>Modified: </b> Train on 100 Real Non-Target Images + 75 Syn Target Images, Test on 100 Target Domain Images </li>
                                    </ul>
                                    </br>

                                </div>


                                <br>
                                <div class="row">
                                    <div class="col-md-6 my-auto">
                                        <img src="assets/img/methodology/dataset%20within%20domain%20process.jpg" class="img-responsive" alt="" style="width:100%">
                                        <caption class="text-muted"><i>Figure: <strong>Within-domain</strong> experiment example using Northwest as target domain.</i></caption>
                                    </div>
                                    <div class="col-md-6 my-auto">
                                        <h5 class="service-heading">Within Domain Experiment (Target = Source)</h5>
                                        <p class="text-muted" style="text-align: left;">
                                            For each of the domains we selected, we ran the baseline and modified experiments, where all of the data came from the same
                                            region. This experiment helps to evaluate the overall ability of synthetic imagery (especially using our GP-GAN technique)
                                            to improve the object detection performance.
                                        </p>
                                    </div>
                                </div>
                                <br>


                                <br>
                                <div class="row">
                                    <div class="col-md-6 my-auto">
                                        <img src="assets/img/methodology/dataset%20cross%20domain%20process.jpg" class="img-responsive" alt="" style="width:100%">
                                        <caption class="text-muted"><i>Figure: <strong>Cross-domain</strong> experiment example using Northwest as target domain and Eastern Midwest as non-target domain.</i></caption>
                                    </div>
                                    <div class="col-md-6 my-auto">
                                        <h5 class="service-heading">Cross Domain Experiment (Target not equal to Source)</h5>
                                        <p class="text-muted" style="text-align: left;">
                                            For these experiments, the domains for the real source and target images are different, while the synthetic images used in the modified training dataset
                                            are from the target region. Thus, with synthetic images more similar to the target region, we hypothesize that the addition of the synthetic images will
                                            improve the accuracy of the object detection when the target and source regions are dissimilar in appearance. These experiments will help us to evaluate
                                            the potential for synthetic imagery to improve the object detection model’s ability to generalize across different regions despite the  limitations of the
                                            existing training data. </p>
                                    </div>
                                </div>
                                <br>

                                <div class="row">
                                    <h5 class="service-heading">YOLOv3</h5>
                                    <div class="row">

                                        <div class="col-md-6 my-auto">
                                            <p class="text-muted" style="text-align: justify;">
                                                A popular object detection model is YOLOv3. It is widely used because of its much faster object detection speed with
                                                similar mAP as other well-performing models. Our previous Bass Connections team also used YOLOv3, which allowed us to make
                                                direct comparisons between the performance of our models and theirs.
                                            </p>
                                        </div>

                                        <div class="col-md-6 my-auto">
                                            <img src="assets/img/methodology/yolov3.png" class="img-responsive" alt="Ratio Test Experimental Design" style="width:100%">
                                            <caption class="text-muted"><i>Figure: Sample output image from <a href="https://github.com/ultralytics/yolov3">Ultralytics YOLOv3 GitHub repository</a>.</i></caption>
                                        </div>
                                    </div>

                            </div>
                        </div>

                    </div>
        </section>

        <!-- Results-->
        <section class="page-section" id="results">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="col-lg-12 text-center">
                        <h2 class="section-heading text-uppercase">Results</h2>
                        <hr class="divider light my-4" />
                        <div class="row">
                            <div class="col-md-6 my-auto">
                                <br>
                                <img src="assets/img/results/metrics.png" class="img-responsive" alt="" style="width:100%">
                                <br>
                                <caption><font size="2">
                                    Figure 1: In this image, the YOLOv3 model predicted that 4 objects were wind turbines. 2 of those predictions were correct, meaning
                                    the precision would be 2/4. There are 3 wind turbines in the image and the model found 2 of these, meaning the
                                    recall would be 2/3.</font></caption>
                            </div>
                            <div class = "col-md-6">
                                <h3>Performance Metrics</h3>
                                <p class="text-muted" style="text-align: justify;">
                                    To understand our results, it's critical that we first understand the metrics that we have chosen to measure
                                    performance. The primary metrics we will use is Average Precision, which combines the classification metrics
                                    of precision and recall. We will explain the implication of these metrics starting with the images on the left.</p>
                                <ul>
                                    <li class="text-muted" style="text-align: justify;"><b>Precision: </b> Out of the areas that the model classified as a wind turbine,
                                        what fraction of these were actually wind turbines. (Positive Predictive Value)</li>
                                    <li class="text-muted" style="text-align: justify;"><b>Recall: </b> Out of wind turbines present in the images (ground truth), what fraction of these
                                        did the model classify as wind turbines (Hit Rate)</li>
                                </ul>
                            </div>
                            <div class="row">
                                <div class = "col-md-6 my-auto">
                                    <p class="text-muted" style="text-align: justify;">
                                        Now we plot the values of precision and recall of the model's predicted outputs  on a graph, which is known as a precision-recall
                                        curve. On the curves to the right, it is evident that that as precision increases, recall decreases, and vice versa. There is hence a tradeoff between
                                        precision and recall. However, we would like to have high values for both precision and recall, which means we would like the area
                                        under the precision-recall curve to be as high as possible. A metric that quantifies this area is Average Precision (AP), and thus
                                        summarizes the precision-recall curve and rewards models with a high precision and recall. </p>
                                    <p class="text-muted" style="text-align: justify;">
                                        In the machine learning space, small absolute increases in AP denote a significant improvement in model performance.
                                    </p>
                                </div>
                                <div class="col-md-6">
                                    <br>
                                    <img src="assets/img/results/pr_curves.png" class="img-responsive" alt="" style="width:100%">
                                    <br>
                                    <caption><font size="2">Figure 2:  Precision Recall Curves. We would like the curve to move to right as much as possible
                                       </font></caption>
                                    <br>
                                    <br>
                                    <br>
                                </div>
                                <div class="row">
                                    <div class="col-md-6 my-auto">
                                        <br>
                                        <img src="assets/img/results/variance.png" class="img-responsive" alt="" style="width:100%">
                                        <br>
                                        <caption><font size="2">Figure 3: Sample PR curves of 4 runs of the same experiment. </font></caption>
                                        <br>
                                        <br>
                                    </div>
                                    <div class = "col-md-6">
                                        <h3>Reducing Variance</h3>
                                        <p class="text-muted" style="text-align: justify;">
                                            Due to variability and stochasticity in the object detection model’s training process, there will be slight variations
                                            between the results of each run, as shown on the left image. Each experiment is therefore repeated 4 times to account
                                            for this randomness and improve the accuracy of the result. The average AP value is calculated and used to compare results
                                            of our baseline model, model with added CityEngine images, and model with added GP GAN images.</p>
                                    </div>
                                    <br>
                                    <hr class="divider light my-4" />
                                    <h2 style="text-align: left;">Results</h2>
                                    <div class="row">
                                        <div class="col-md-12 text-center">
                                            <p class="text-muted" style="text-align: justify;">
                                                The performances of the model with added synthetic images <b>improve significantly </b> in both within-domain and cross-domain settings.
                                                Synthetic images are especially helpful in cross-domain settings, which means they can be useful when there is a lack of data or when it
                                                is cost-prohibitive to collect data of the target domain.
                                            </p>
                                        </div>
                                        <div class="col-md-12 my-auto">
                                            <img src="assets/img/results/overall_graph.png" class="img-responsive" alt="" style="width:80%">
                                        </div>
                                        <div class="col-md-12 my-auto">
                                            <img src="assets/img/results/overall_table.png" class="img-responsive" alt="" style="width:70%">
                                            <br>
                                            <caption><font size="2">Figure 4: All values are in average precision (AP). </font></caption>
                                            <br><br><br>
                                        </div>

                                    </div>

                                    <div class="row">
                                        <div class="col-md-6">
                                            <img src="assets/img/results/test%20batch%20gt.jpg" class="img-responsive" alt="" style="width:80%">
                                            <br>
                                            <caption><font size="2">Figure 5: Sample ground truth images for testing batch from the GP GAN experiment of training on Northeast and testing on Northeast.  </font></caption>
                                        </div>
                                        <div class="col-md-6">
                                            <img src="assets/img/results/test%20batch%20pred.jpg" class="img-responsive" alt="" style="width:80%">
                                            <br>
                                            <caption><font size="2">Figure 6: Sample predictions for testing batch from the GP GAN experiment of training on Northeast and testing on Northeast.
                                                The YOLOv3 model achieves a precision of 17/18 or 0.94 (one of the predicted outputs in the bottom middle left is not a turbine)
                                                and recall of 17/19 or 0.89 (misses a wind turbine in the top middle left and bottom middle right) </font></caption>
                                            <br>
                                            <br>
                                        </div>
                                        <div class="col-md-1"></div>
                                    </div>
                                    <hr class="divider light my-4" />
                                    <h2 style="text-align: left;">Results of Each Geographic Domain Respectively</h2>
                                    <div class="row">
                                        <div class="col-md-12 text-center">
                                            <p class="text-muted" style="text-align: justify;">
                                                Here we will present a closer look into the results of training with real images from each of the 3 geographic regions respectively.
                                                There is a disparity in performance when the model is trained with real images of different geographic domains. In particular, in
                                                cross-domain experiments that test on Eastern Midwest, the model performs generally worse than when testing on other regions.
                                            </p>
                                        </div>
                                    </div>
                                    <div class="row">
                                        <div class="col-md-6 my-auto">
                                            <img src="assets/img/results/NE_graph.png" class="img-responsive" alt="" style="width:100%">
                                            <img src="assets/img/results/NE_table.png" class="img-responsive" alt="" style="width:100%">
                                            <br>
                                            <caption><font size="2">Figure 7: Results of training with real images from Northeast. </font></caption>
                                            <br>
                                            <br>
                                            <br>
                                            <br>
                                        </div>
                                        <div class="col-md-6 my-auto">
                                            <img src="assets/img/results/EM_graph.png" class="img-responsive" alt="" style="width:100%">
                                            <img src="assets/img/results/EM_table.png" class="img-responsive" alt="" style="width:100%">
                                            <br>
                                            <caption><font size="2">Figure 8: Results of training with real images from Northwest. </font></caption>
                                            <br>
                                            <br>
                                            <br>
                                            <br>
                                        </div>
                                    </div>

                                    <p class="text-muted" style="text-align: justify;">
                                        As shown above, the model performs consistently worse in the cross-domain experience. However, the model
                                        has the greatest average improvement in average precision from the addition of the GP GAN in these same cross-domain experiments, improving the overall cross-domain performance by 31% from the baseline.
                                        In fact, the effect of the GP GAN is greatly noticed when considering the worst performance of each dataset. The GP GAN's worst performance of
                                        0.638 Average Precision on the Train EM Val NE experiment is much greater than the other models worst performances, providing a sharp increase in performance.
                                        Thus, it provides promise for bridging the gap in cross-domain experiments for different geographic regions.
                                    </p>

                                    <!--<div class="row">
                                        <div class="col-md-6 my-auto">
                                            <img src="assets/img/results/EM_graph.png" class="img-responsive" alt="" style="width:100%">
                                            <img src="assets/img/results/EM_table.png" class="img-responsive" alt="" style="width:100%">
                                            <caption><font size="2">Figure 9: Results of training with real images from Eastern Midwest</font></caption>
                                        </div>
                                        <div class="col-md-6">
                                            <p class="text-muted" style="text-align: center;">
                                                As shown above, the model performs consistently worse when testing on Eastern Midwest, yet Eastern Midwest is also the region where the model
                                                has the greatest average improvement in average precision from the addition of synthetic imagery. The exact model performance and improvement synthetic images can make hence depend on the details of specific geographic regions. <br><br> It is therefore possibly more challenging for the model to perform well in certain settings, such as when there are different designs of wind turbines in a region or when the region has more diverse geographic backgrounds. Despite these challenges, synthetic imagery was able to help bridge the gap and bring significant improvement to model performance. </p>
                                        </div>
                                    </div>-->
        </section>


        <!-- Key Takeaways-->
        <section class="page-section" id="key-takeaways">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="col-lg-12 text-center">
                        <h2 class="section-heading text-uppercase">Key Takeaways</h2>

                        <hr class="divider light my-4" />
                        <div class="row">
                            <div class="col-md-1 my-auto"></div>
                            <div class="col-md-10 my-auto">
                                <p class="text-muted" style="text-align: justify;">The results show that adding the curated GP GAN generated imagery improves the performance of
                                    our object detection model in all cases. This is especially the case in cross domain experiments (testing on an unseen region).
                                    The performance increase is more limited in the within domain setting, where there the model is testing on a previously seen region and was already generally
                                    performing well. Furthermore, our model not only improves upon the baseline, but also the synthetic CityEngine dataset, demonstrating
                                    its ability to outperform other methods of synthetic image generation, especially in cross-domain experiments.
                                    Given that our method of synthetic image generation is free and quick to produce, it evidently presents a simple and effective
                                    method of enhancing object detection model performance on new domains. Furthermore, it can serve to supplement datasets that simply we lack training data, which is often the
                                    case when we are trying to obtain information on energy infrastructure. With the aid of our synthetic imagery, this method of identifying and gathering
                                    locations of energy infrastructure in a geographic region could bridge the information gaps that energy access planners need when making decisions about electrification.</p>
                            </div>
                            <div class="col-md-1 my-auto"></div>
                        </div>
                        <br>
                        <div class="row">
                            <div class="col-md-6 my-auto">
                                <img src="assets/img/key_takeaways/takeaways_graph.png" class="img-responsive" alt="" style="width:100%">
                            </div>
                            <div class="col-md-6 my-auto">
                                <img src="assets/img/key_takeaways/takeaways_table.png" class="img-responsive" alt="" style="width:100%">
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>


        <!-- Future Work -->
        <section class="page-section" id="future-work">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="col-lg-12 text-center">
                        <h2 class="section-heading text-uppercase">Future Work</h2>
                        <hr class="divider light my-4" />
                        <div class="row">
                            <div class="col-md-12 my-auto">
                                <ol class="text-muted" style="text-align: justify;">
                                    <li>
                                        Given various time constraints in our project, we would like to attempt more hyperparameter tuning in the
                                        image augmentation process to ensure the images come from a realistic sizing distribution and blending processes
                                        to improve the realisticness of our generated images and increase blending.
                                    </li>
                                    <br>
                                    <li>
                                        Apply these techniques to <strong>detect other types of energy infrastructure</strong>. Because high voltage transmission towers are
                                        similar structure to wind turbines, we can easily adapt our synthetic image generation process for transmission towers. We could then
                                        test this synthetic imagery in the same manner as what we performed for the synthetic imagery of wind turbines, and see if this method
                                        extends to this other types of energy infrastructure.
                                    </li>
                                    <br>
                                    <div class="figure-img">
                                        <img src="assets/img/future_work/energyinfrastructure.png" class="img-responsive" alt="" style="width:100%">
                                        <br>
                                        <caption><i>Images from Pixabay: <a href="https://pixabay.com/images/id-3196696/">Substation</a>,
                                            <a href="https://pixabay.com/images/id-1868352/">Transmission Tower</a>,
                                            <a href="https://pixabay.com/images/id-2138992/">Solar Panels</a></i></caption>
                                    </div>
                                    <br>
                                    <li>
                                        We would like to investigate the performance of our synthetic dataset on the YOLOv4 object detection model. YOLOv4 has demonstrated that
                                        its AP (Average Precision) has increased by 10% since the YOLOv3 model. Thus, to further improve the wind turbine detection in in-domain
                                        and cross-domain experiments, further experimentation with our real and synthetic datasets should be performed with the newer, state-of-the-art
                                        object detection model. Additionally, cross-validation of our synthetic imagery techniques using different object detection models including YOLOv4 and EfficientDet could strengthen our results.
                                    </li>
                                    <br>
                                    <div class= "figure-img">
                                        <img src="assets/img/future_work/yolov4.png" class="img-responsive" alt="" style="width:100%">
                                        <br>
                                        <caption><i>Alexey Bochkovskiy, Chien-Yao Wang, and HongYuan Mark Liao. Yolov4: Optimal speed and accuracy of
                                            object detection. arXiv preprint arXiv:2004.10934, 2020.</i></caption>
                                    </div>
                                    <li>
                                        Investigate few shot learning, where we use <strong>small amounts of real images and large amounts of synthetic data</strong> to adapt our object detection
                                        model to any region that we choose.
                                    </li>
                                    <br>
                                    <div class= "figure-img">
                                        <img src="assets/img/future_work/Few Shot Learning.png" class="img-responsive" alt="" style="width:100%">
                                        <br>
                                        <caption><i>Figure: Guo, Y., Codella, N., Karlinsky, L., Smith, J., Simunic, T., & Feris, R. (2019). A New Benchmark for Evaluation of Cross-Domain Few-Shot Learning. ArXiv, abs/1912.07200. </i></caption>
                                    </div>
                                </ol>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Acknowledgements -->
        <section class="bg-light page-section" id="acknowledgements">
            <div class="container">
                <div class="row">
                    <div class="col-lg-12 text-center">
                        <h2 class="section-heading text-uppercase">Acknowledgements</h2>
                    </div>
                </div>
                <br>
                <div class="row justify-content-center">
                    <p class="text-muted" style="text-align: justify;"> We would like to thank Dr. Kyle Bradbury, Dr. Jordan Malof, and Wayne Hu for their
                        help and guidance along the way. We would also like to thank the previous Bass Connections and Data+ teams for their work leading up
                        to this project. We would also like to thank Dr. Paul Bendich and Dr. Greg Herschlag for their work organizing and hosting
                        the Duke Data Plus talks, and the speakers who shared their wisdom about the field of data science.
                        Thank you to the <strong><a href="https://bigdata.duke.edu/data">Duke Data Plus</a></strong> and
                        <strong><a href="https://energy.duke.edu">Duke Energy Initiative</a></strong> that supported this project.
                    </p>
                </div>
            </div>
        </section>
                <!-- Services-->
        <!--<section class="page-section" id="services">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">Services</h2>
                    <h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3>
                </div>
                <div class="row text-center">
                    <div class="col-md-4">
                        <span class="fa-stack fa-4x">
                            <i class="fas fa-circle fa-stack-2x text-primary"></i>
                            <i class="fas fa-shopping-cart fa-stack-1x fa-inverse"></i>
                        </span>
                        <h4 class="my-3">E-Commerce</h4>
                        <p class="text-muted">Lorem ipsum dolor sit amet, consectetur adipisicing elit. Minima maxime quam architecto quo inventore harum ex magni, dicta impedit.</p>
                    </div>
                    <div class="col-md-4">
                        <span class="fa-stack fa-4x">
                            <i class="fas fa-circle fa-stack-2x text-primary"></i>
                            <i class="fas fa-laptop fa-stack-1x fa-inverse"></i>
                        </span>
                        <h4 class="my-3">Responsive Design</h4>
                        <p class="text-muted">Lorem ipsum dolor sit amet, consectetur adipisicing elit. Minima maxime quam architecto quo inventore harum ex magni, dicta impedit.</p>
                    </div>
                    <div class="col-md-4">
                        <span class="fa-stack fa-4x">
                            <i class="fas fa-circle fa-stack-2x text-primary"></i>
                            <i class="fas fa-lock fa-stack-1x fa-inverse"></i>
                        </span>
                        <h4 class="my-3">Web Security</h4>
                        <p class="text-muted">Lorem ipsum dolor sit amet, consectetur adipisicing elit. Minima maxime quam architecto quo inventore harum ex magni, dicta impedit.</p>
                    </div>
                </div>
            </div>
        </section>-->
        <!-- Portfolio Grid-->

        <!--<section class="page-section bg-light" id="portfolio">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">Portfolio</h2>
                    <h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3>
                </div>
                <div class="row">
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal1">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/1.jpg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Threads</div>
                                <div class="portfolio-caption-subheading text-muted">Illustration</div>
                            </div>
                        </div>
                    </div>
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal2">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/2.jpg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Explore</div>
                                <div class="portfolio-caption-subheading text-muted">Graphic Design</div>
                            </div>
                        </div>
                    </div>
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal3">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/3.jpg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Finish</div>
                                <div class="portfolio-caption-subheading text-muted">Identity</div>
                            </div>
                        </div>
                    </div>
                    <div class="col-lg-4 col-sm-6 mb-4 mb-lg-0">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal4">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/4.jpg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Lines</div>
                                <div class="portfolio-caption-subheading text-muted">Branding</div>
                            </div>
                        </div>
                    </div>
                    <div class="col-lg-4 col-sm-6 mb-4 mb-sm-0">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal5">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/5.jpg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Southwest</div>
                                <div class="portfolio-caption-subheading text-muted">Website Design</div>
                            </div>
                        </div>
                    </div>
                    <div class="col-lg-4 col-sm-6">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal6">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/6.jpg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Window</div>
                                <div class="portfolio-caption-subheading text-muted">Photography</div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>-->
        <!-- About-->
        <!--<section class="page-section" id="about">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">About</h2>
                    <h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3>
                </div>
                <ul class="timeline">
                    <li>
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/about/1.jpg" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h4>2009-2011</h4>
                                <h4 class="subheading">Our Humble Beginnings</h4>
                            </div>
                            <div class="timeline-body"><p class="text-muted">Lorem ipsum dolor sit amet, consectetur adipisicing elit. Sunt ut voluptatum eius sapiente, totam reiciendis temporibus qui quibusdam, recusandae sit vero unde, sed, incidunt et ea quo dolore laudantium consectetur!</p></div>
                        </div>
                    </li>
                    <li class="timeline-inverted">
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/about/2.jpg" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h4>March 2011</h4>
                                <h4 class="subheading">An Agency is Born</h4>
                            </div>
                            <div class="timeline-body"><p class="text-muted">Lorem ipsum dolor sit amet, consectetur adipisicing elit. Sunt ut voluptatum eius sapiente, totam reiciendis temporibus qui quibusdam, recusandae sit vero unde, sed, incidunt et ea quo dolore laudantium consectetur!</p></div>
                        </div>
                    </li>
                    <li>
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/about/3.jpg" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h4>December 2015</h4>
                                <h4 class="subheading">Transition to Full Service</h4>
                            </div>
                            <div class="timeline-body"><p class="text-muted">Lorem ipsum dolor sit amet, consectetur adipisicing elit. Sunt ut voluptatum eius sapiente, totam reiciendis temporibus qui quibusdam, recusandae sit vero unde, sed, incidunt et ea quo dolore laudantium consectetur!</p></div>
                        </div>
                    </li>
                    <li class="timeline-inverted">
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/about/4.jpg" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h4>July 2020</h4>
                                <h4 class="subheading">Phase Two Expansion</h4>
                            </div>
                            <div class="timeline-body"><p class="text-muted">Lorem ipsum dolor sit amet, consectetur adipisicing elit. Sunt ut voluptatum eius sapiente, totam reiciendis temporibus qui quibusdam, recusandae sit vero unde, sed, incidunt et ea quo dolore laudantium consectetur!</p></div>
                        </div>
                    </li>
                    <li class="timeline-inverted">
                        <div class="timeline-image">
                            <h4>
                                Be Part
                                <br />
                                Of Our
                                <br />
                                Story!
                            </h4>
                        </div>
                    </li>
                </ul>
            </div>
        </section>-->
        <!-- Team-->
        <section class="page-section bg-light" id="team">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">Our Amazing Team</h2>
                    <h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3>
                </div>
                <div class="row">
                    <div class="col-lg-4">
                        <div class="team-member">
                            <img class="mx-auto rounded-circle" src="assets/img/team/Frankie_Portrait.jpg" alt="..." />
                            <h4>Frankie Willard</h4>
                            <p class="text-muted">Data Scientist</p>
                            <!--<a class="btn btn-dark btn-social mx-2" href="#!"><i class="fab fa-twitter"></i></a>
                            <a class="btn btn-dark btn-social mx-2" href="#!"><i class="fab fa-facebook-f"></i></a>-->
                            <a class="btn btn-dark btn-social mx-2" href="https://github.com/frankwillard"><i class="fab fa-github"></i></a>
                            <a class="btn btn-dark btn-social mx-2" href="https://www.linkedin.com/in/frank-willard"><i class="fab fa-linkedin-in"></i></a>
                        </div>
                    </div>
                    <div class="col-lg-4">
                        <div class="team-member">
                            <img class="mx-auto rounded-circle" src="assets/img/team/alex.jpg" alt="..." />
                            <h4>Alex Kumar</h4>
                            <p class="text-muted">Data Scientist</p>
                            <!--<a class="btn btn-dark btn-social mx-2" href="#!"><i class="fab fa-twitter"></i></a>
                            <a class="btn btn-dark btn-social mx-2" href="#!"><i class="fab fa-facebook-f"></i></a>-->
                            <a class="btn btn-dark btn-social mx-2" href="https://github.com/ACK101101"><i class="fab fa-github"></i></a>
                            <a class="btn btn-dark btn-social mx-2" href="https://www.linkedin.com/in/alex-kumar-664103160/"><i class="fab fa-linkedin-in"></i></a>
                        </div>
                    </div>
                    <div class="col-lg-4">
                        <div class="team-member">
                            <img class="mx-auto rounded-circle" src="assets/img/team/caroline.jpeg" alt="..." />
                            <h4>Caroline Tang</h4>
                            <p class="text-muted">Data Scientist</p>
                            <!--<a class="btn btn-dark btn-social mx-2" href="#!"><i class="fab fa-twitter"></i></a>
                            <a class="btn btn-dark btn-social mx-2" href="#!"><i class="fab fa-facebook-f"></i></a>-->
                            <a class="btn btn-dark btn-social mx-2" href="https://github.com/caroline98tang"><i class="fab fa-github"></i></a>
                            <a class="btn btn-dark btn-social mx-2" href="https://www.linkedin.com/in/caroline-tang-15874a1a4/"><i class="fab fa-linkedin-in"></i></a>
                        </div>
                    </div>
                </div>
                <div class="row">
                    <div class="col-lg-4">
                        <div class="team-member">
                            <img class="mx-auto rounded-circle" src="assets/img/team/Kyle.png" alt="..." />
                            <h4>Kyle Bradbury</h4>
                            <p class="text-muted">Project Lead</p>
                            <!--<a class="btn btn-dark btn-social mx-2" href="#!"><i class="fab fa-twitter"></i></a>
                            <a class="btn btn-dark btn-social mx-2" href="#!"><i class="fab fa-facebook-f"></i></a>-->
                            <a class="btn btn-dark btn-social mx-2" href="https://github.com/kylebradbury"><i class="fab fa-github"></i></a>
                            <a class="btn btn-dark btn-social mx-2" href="https://www.linkedin.com/in/bradburykyle/"><i class="fab fa-linkedin-in"></i></a>
                        </div>
                    </div>
                    <div class="col-lg-4">
                        <div class="team-member">
                            <img class="mx-auto rounded-circle" src="assets/img/team/Jordan.png" alt="..." />
                            <h4>Jordan Malof</h4>
                            <p class="text-muted">Project Lead</p>
                            <!--<a class="btn btn-dark btn-social mx-2" href="#!"><i class="fab fa-twitter"></i></a>
                            <a class="btn btn-dark btn-social mx-2" href="#!"><i class="fab fa-facebook-f"></i></a>-->
                            <a class="btn btn-dark btn-social mx-2" href="#!"><i class="fab fa-github"></i></a>
                            <a class="btn btn-dark btn-social mx-2" href="https://www.linkedin.com/in/jordanmalof/"><i class="fab fa-linkedin-in"></i></a>
                        </div>
                    </div>
                    <div class="col-lg-4">
                        <div class="team-member">
                            <img class="mx-auto rounded-circle" src="assets/img/team/Wayne.jpg" alt="..." />
                            <h4>Wayne Hu</h4>
                            <p class="text-muted">Project Manager</p>
                            <!--<a class="btn btn-dark btn-social mx-2" href="#!"><i class="fab fa-twitter"></i></a>
                            <a class="btn btn-dark btn-social mx-2" href="#!"><i class="fab fa-facebook-f"></i></a>-->
                            <a class="btn btn-dark btn-social mx-2" href="https://github.com/waynehuu"><i class="fab fa-github"></i></a>
                            <a class="btn btn-dark btn-social mx-2" href="https://www.linkedin.com/in/weihuwayne/"><i class="fab fa-linkedin-in"></i></a>
                        </div>
                    </div>
                </div>
                <!--<div class="row">
                    <div class="col-lg-8 mx-auto text-center"><p class="large text-muted">Lorem ipsum dolor sit amet, consectetur adipisicing elit. Aut eaque, laboriosam veritatis, quos non quis ad perspiciatis, totam corporis ea, alias ut unde.</p></div>
                </div>-->
            </div>
        </section>
        <!-- Clients-->
        <!--<div class="py-5">
            <div class="container">
                <div class="row align-items-center">
                    <div class="col-md-3 col-sm-6 my-3">
                        <a href="#!"><img class="img-fluid img-brand d-block mx-auto" src="assets/img/logos/microsoft.svg" alt="..." /></a>
                    </div>
                    <div class="col-md-3 col-sm-6 my-3">
                        <a href="#!"><img class="img-fluid img-brand d-block mx-auto" src="assets/img/logos/google.svg" alt="..." /></a>
                    </div>
                    <div class="col-md-3 col-sm-6 my-3">
                        <a href="#!"><img class="img-fluid img-brand d-block mx-auto" src="assets/img/logos/facebook.svg" alt="..." /></a>
                    </div>
                    <div class="col-md-3 col-sm-6 my-3">
                        <a href="#!"><img class="img-fluid img-brand d-block mx-auto" src="assets/img/logos/ibm.svg" alt="..." /></a>
                    </div>
                </div>
            </div>
        </div>-->

        <!-- Footer-->
        <footer class="footer py-4">
            <div class="container">
                <div class="row align-items-center">
                    <div class="col-lg-4 text-lg-start">Copyright &copy; Your Website 2021</div>
                    <div class="col-lg-4 my-3 my-lg-0">
                        <a class="btn btn-dark btn-social mx-2" href="#!"><i class="fab fa-twitter"></i></a>
                        <a class="btn btn-dark btn-social mx-2" href="#!"><i class="fab fa-facebook-f"></i></a>
                        <a class="btn btn-dark btn-social mx-2" href="#!"><i class="fab fa-linkedin-in"></i></a>
                    </div>
                    <div class="col-lg-4 text-lg-end">
                        <a class="link-dark text-decoration-none me-3" href="#!">Privacy Policy</a>
                        <a class="link-dark text-decoration-none" href="#!">Terms of Use</a>
                    </div>
                </div>
            </div>
        </footer>
        <!-- Portfolio Modals-->
        <!-- Portfolio item 1 modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal1" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    <!-- Project details-->
                                    <h2 class="text-uppercase">Project Name</h2>
                                    <p class="item-intro text-muted">Lorem ipsum dolor sit amet consectetur.</p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/1.jpg" alt="..." />
                                    <p>Use this area to describe your project. Lorem ipsum dolor sit amet, consectetur adipisicing elit. Est blanditiis dolorem culpa incidunt minus dignissimos deserunt repellat aperiam quasi sunt officia expedita beatae cupiditate, maiores repudiandae, nostrum, reiciendis facere nemo!</p>
                                    <ul class="list-inline">
                                        <li>
                                            <strong>Client:</strong>
                                            Threads
                                        </li>
                                        <li>
                                            <strong>Category:</strong>
                                            Illustration
                                        </li>
                                    </ul>
                                    <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                        <i class="fas fa-times me-1"></i>
                                        Close Project
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Portfolio item 2 modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal2" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    <!-- Project details-->
                                    <h2 class="text-uppercase">Project Name</h2>
                                    <p class="item-intro text-muted">Lorem ipsum dolor sit amet consectetur.</p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/2.jpg" alt="..." />
                                    <p>Use this area to describe your project. Lorem ipsum dolor sit amet, consectetur adipisicing elit. Est blanditiis dolorem culpa incidunt minus dignissimos deserunt repellat aperiam quasi sunt officia expedita beatae cupiditate, maiores repudiandae, nostrum, reiciendis facere nemo!</p>
                                    <ul class="list-inline">
                                        <li>
                                            <strong>Client:</strong>
                                            Explore
                                        </li>
                                        <li>
                                            <strong>Category:</strong>
                                            Graphic Design
                                        </li>
                                    </ul>
                                    <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                        <i class="fas fa-times me-1"></i>
                                        Close Project
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Portfolio item 3 modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal3" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    <!-- Project details-->
                                    <h2 class="text-uppercase">Project Name</h2>
                                    <p class="item-intro text-muted">Lorem ipsum dolor sit amet consectetur.</p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/3.jpg" alt="..." />
                                    <p>Use this area to describe your project. Lorem ipsum dolor sit amet, consectetur adipisicing elit. Est blanditiis dolorem culpa incidunt minus dignissimos deserunt repellat aperiam quasi sunt officia expedita beatae cupiditate, maiores repudiandae, nostrum, reiciendis facere nemo!</p>
                                    <ul class="list-inline">
                                        <li>
                                            <strong>Client:</strong>
                                            Finish
                                        </li>
                                        <li>
                                            <strong>Category:</strong>
                                            Identity
                                        </li>
                                    </ul>
                                    <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                        <i class="fas fa-times me-1"></i>
                                        Close Project
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Portfolio item 4 modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal4" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    <!-- Project details-->
                                    <h2 class="text-uppercase">Project Name</h2>
                                    <p class="item-intro text-muted">Lorem ipsum dolor sit amet consectetur.</p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/4.jpg" alt="..." />
                                    <p>Use this area to describe your project. Lorem ipsum dolor sit amet, consectetur adipisicing elit. Est blanditiis dolorem culpa incidunt minus dignissimos deserunt repellat aperiam quasi sunt officia expedita beatae cupiditate, maiores repudiandae, nostrum, reiciendis facere nemo!</p>
                                    <ul class="list-inline">
                                        <li>
                                            <strong>Client:</strong>
                                            Lines
                                        </li>
                                        <li>
                                            <strong>Category:</strong>
                                            Branding
                                        </li>
                                    </ul>
                                    <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                        <i class="fas fa-times me-1"></i>
                                        Close Project
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Portfolio item 5 modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal5" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    <!-- Project details-->
                                    <h2 class="text-uppercase">Project Name</h2>
                                    <p class="item-intro text-muted">Lorem ipsum dolor sit amet consectetur.</p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/5.jpg" alt="..." />
                                    <p>Use this area to describe your project. Lorem ipsum dolor sit amet, consectetur adipisicing elit. Est blanditiis dolorem culpa incidunt minus dignissimos deserunt repellat aperiam quasi sunt officia expedita beatae cupiditate, maiores repudiandae, nostrum, reiciendis facere nemo!</p>
                                    <ul class="list-inline">
                                        <li>
                                            <strong>Client:</strong>
                                            Southwest
                                        </li>
                                        <li>
                                            <strong>Category:</strong>
                                            Website Design
                                        </li>
                                    </ul>
                                    <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                        <i class="fas fa-times me-1"></i>
                                        Close Project
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Portfolio item 6 modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal6" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    <!-- Project details-->
                                    <h2 class="text-uppercase">Project Name</h2>
                                    <p class="item-intro text-muted">Lorem ipsum dolor sit amet consectetur.</p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/6.jpg" alt="..." />
                                    <p>Use this area to describe your project. Lorem ipsum dolor sit amet, consectetur adipisicing elit. Est blanditiis dolorem culpa incidunt minus dignissimos deserunt repellat aperiam quasi sunt officia expedita beatae cupiditate, maiores repudiandae, nostrum, reiciendis facere nemo!</p>
                                    <ul class="list-inline">
                                        <li>
                                            <strong>Client:</strong>
                                            Window
                                        </li>
                                        <li>
                                            <strong>Category:</strong>
                                            Photography
                                        </li>
                                    </ul>
                                    <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                        <i class="fas fa-times me-1"></i>
                                        Close Project
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
        <!-- * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *-->
        <!-- * *                               SB Forms JS                               * *-->
        <!-- * * Activate your form at https://startbootstrap.com/solution/contact-forms * *-->
        <!-- * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *-->
        <script src="https://cdn.startbootstrap.com/sb-forms-latest.js"></script>
    </body>
</html>
